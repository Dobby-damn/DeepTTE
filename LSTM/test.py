# incremental_training.py

"""
å¢é‡è®­ç»ƒ 5 å±‚ LSTM è„šæœ¬
======================

æ•°æ®ï¼š
- D:/Code/DeepTTE/LSTM/è½¨è¿¹æµ‹è¯•æ•°æ®(1).csv    (~1â€¯000â€¯000 è¡Œ)
- D:/Code/DeepTTE/LSTM/è½¨è¿¹æµ‹è¯•æ•°æ®(2).xlsx  (~20â€¯000 è¡Œ)
- D:/Code/DeepTTE/LSTM/iddiag.csv           (evaluation_id, diagnose)

åŠŸèƒ½ï¼š
1. æŒ‰ chunk é€å—è¯»å…¥å¤§ CSVï¼Œåˆå¹¶è¯Šæ–­æ ‡ç­¾ï¼Œè®­ç»ƒåä¿å­˜ checkpoint
2. æŠŠ XLSX æ–‡ä»¶å½“éªŒè¯é›†ï¼Œä¸€æ¬¡æ€§åŠ è½½å¹¶è¯„ä¼°
3. æ¯ä¸ª chunk è®­ç»ƒåï¼šè®¡ç®— train lossã€val lossã€val accï¼›ä¿å­˜åˆ° history
4. è®­ç»ƒç»“æŸè‡ªåŠ¨è¾“å‡ºä¸¤å¼ æ›²çº¿å›¾ï¼ˆloss & accuracyï¼‰
5. æ”¯æŒä»æŸä¸ª checkpoint æ¢å¤è®­ç»ƒ
"""

import os
import json
from collections import defaultdict

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import LabelEncoder

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ CONFIG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

CSV_PATH      = r"D:/Code/DeepTTE/LSTM/è½¨è¿¹æµ‹è¯•æ•°æ®(1).csv"
XLSX_PATH     = r"D:/Code/DeepTTE/LSTM/è½¨è¿¹æµ‹è¯•æ•°æ®(2).xlsx"
IDDIAG_PATH   = r"D:/Code/DeepTTE/LSTM/iddiag.csv"

CHUNK_SIZE    = 100_000    # CSV æŒ‰è¡Œæ•°åˆ†å—
MAX_SEQ_LEN   = 150        # æ¯æ¡è½¨è¿¹ pad/truncate é•¿åº¦
BATCH_SIZE    = 64
LR            = 1e-3
HIDDEN_SIZE   = 64
NUM_LAYERS    = 5

CHECKPOINT_DIR = "checkpoints"
HISTORY_PLOT   = "training_history.png"
RESUME_PATH    = None       # e.g. "checkpoints/ckpt_chunk3.pt" æ¥æ¢å¤

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
SEED   = 42

torch.manual_seed(SEED)
np.random.seed(SEED)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ å·¥å…·å‡½æ•° & ç±» â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def pad_sequence(arr: np.ndarray, length: int = MAX_SEQ_LEN) -> np.ndarray:
    """Pad or truncate (N,2) array to (length,2)."""
    if arr.shape[0] >= length:
        return arr[:length]
    pad = np.zeros((length - arr.shape[0], 2), dtype=arr.dtype)
    return np.vstack([arr, pad])

def build_sequences(df: pd.DataFrame, id_col: str = "evaluation_id"): #-> tuple[list[np.ndarray], list[int]]:
    """
    æŒ‰å—è¯•è€…åˆ†ç»„ï¼Œè¿”å› list of (x,y)-ndarrays å’Œ int labelsã€‚
    éœ€ df åŒ…å«åˆ—ï¼š[id_col, "x","y","label"]ã€‚
    """
    seqs, lbls = [], []
    for pid, grp in df.groupby(id_col, sort=False):
        lbl = grp["label"].iloc[0]
        # å¦‚æœ label ä¸¢å¤± (NaN)ï¼Œè·³è¿‡æ­¤å—è¯•è€…
        if pd.isna(lbl):
            continue
        xy = grp[["x","y"]].values.astype(np.float32)
        seqs.append(pad_sequence(xy, MAX_SEQ_LEN))
        lbls.append(int(lbl))
    return seqs, lbls

class TouchDataset(Dataset):
    """Wrap sequences & labels into PyTorch Dataset."""
    def __init__(self, seqs: list[np.ndarray], labels: list[int]):
        self.X = torch.tensor(np.stack(seqs), dtype=torch.float32)
        self.y = torch.tensor(labels, dtype=torch.long)
    def __len__(self):
        return len(self.X)
    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

class LSTMClassifier(nn.Module):
    """5-layer LSTM + 1-layer FC."""
    def __init__(self, input_size: int, hidden_size: int, num_layers: int, num_classes: int):
        super().__init__()
        self.lstm = nn.LSTM(input_size, hidden_size,
                            num_layers=num_layers,
                            batch_first=True)
        self.fc   = nn.Linear(hidden_size, num_classes)
    def forward(self, x):
        # x: (B, T, 2)
        out, _ = self.lstm(x)            # (B, T, hidden)
        last = out[:, -1, :]             # (B, hidden)
        return self.fc(last)             # (B, num_classes)

def evaluate(model: nn.Module, loader: DataLoader, criterion) :#-> tuple[float, float]:
    """è¿”å› (avg_loss, accuracy%) on loader."""
    model.eval()
    total_loss, correct, total = 0.0, 0, 0
    with torch.no_grad():
        for xb, yb in loader:
            xb, yb = xb.to(DEVICE), yb.to(DEVICE)
            logits = model(xb)
            loss = criterion(logits, yb)
            total_loss += loss.item()
            preds = logits.argmax(dim=1)
            correct += (preds == yb).sum().item()
            total += yb.size(0)
    return total_loss / len(loader), 100 * correct / total

def save_checkpoint(model, optim, chunk_idx: int, history: dict):
    os.makedirs(CHECKPOINT_DIR, exist_ok=True)
    ckpt = {
        "model_state": model.state_dict(),
        "optim_state": optim.state_dict(),
        "history":     history,
        "chunk_idx":   chunk_idx
    }
    path = os.path.join(CHECKPOINT_DIR, f"ckpt_chunk{chunk_idx}.pt")
    torch.save(ckpt, path)
    with open(os.path.join(CHECKPOINT_DIR, "history.json"), "w", encoding="utf-8") as f:
        json.dump(history, f, ensure_ascii=False, indent=2)

def plot_history(history: dict, out_fn: str):
    epochs = list(range(1, len(history["train_loss"]) + 1))
    plt.figure()
    plt.plot(epochs, history["train_loss"], label="Train loss")
    plt.plot(epochs, history["val_loss"],   label="Val loss")
    plt.xlabel("Chunk #")
    plt.ylabel("Loss")
    plt.legend()
    plt.tight_layout()
    plt.savefig(out_fn)
    plt.close()

    plt.figure()
    plt.plot(epochs, history["val_acc"], label="Val acc (%)")
    plt.xlabel("Chunk #")
    plt.ylabel("Accuracy (%)")
    plt.ylim(0, 100)
    plt.tight_layout()
    plt.savefig(out_fn.replace(".png","_acc.png"))
    plt.close()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ä¸»æµç¨‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def main():
    os.makedirs(CHECKPOINT_DIR, exist_ok=True)

    # 1ï¸âƒ£ è¯»å–è¯Šæ–­æ ‡ç­¾ï¼Œæ„å»º LabelEncoder
    diag_df = pd.read_csv(IDDIAG_PATH, usecols=["evaluation_id","diagnose"])
    label_encoder = LabelEncoder().fit([0,1])  # 0=æ­£å¸¸,1=å¼‚å¸¸
    num_classes = len(label_encoder.classes_)
    print(f"æ£€æµ‹åˆ° {num_classes} ç±»æ ‡ç­¾ï¼š{label_encoder.classes_}")

    # 2ï¸âƒ£ æ„å»ºéªŒè¯é›†ï¼ˆXLSXï¼‰
    df_val = pd.read_excel(XLSX_PATH, engine="openpyxl") \
               .rename(columns={"ex":"x","ey":"y"}) \
               .merge(diag_df, on="evaluation_id", how="inner") \
               .rename(columns={"diagnose":"label"})
    val_seqs, val_lbls_raw = build_sequences(df_val, id_col="evaluation_id")
    val_lbls = label_encoder.transform(val_lbls_raw)
    val_loader = DataLoader(TouchDataset(val_seqs, val_lbls), batch_size=BATCH_SIZE)
    print(f"éªŒè¯é›†ï¼š{len(val_seqs)} æ¡åºåˆ—")

    # 3ï¸âƒ£ æ¨¡å‹ & ä¼˜åŒ–å™¨ & æŸå¤± & history
    model     = LSTMClassifier(input_size=2,
                               hidden_size=HIDDEN_SIZE,
                               num_layers=NUM_LAYERS,
                               num_classes=num_classes).to(DEVICE)
    optimizer = torch.optim.Adam(model.parameters(), lr=LR)
    criterion = nn.CrossEntropyLoss()
    history   = defaultdict(list)
    start_chunk = 0

    # 3bï¸âƒ£ å¦‚æœè¦æ¢å¤è®­ç»ƒ
    if RESUME_PATH and os.path.exists(RESUME_PATH):
        ckpt = torch.load(RESUME_PATH, map_location=DEVICE)
        model.load_state_dict(ckpt["model_state"])
        optimizer.load_state_dict(ckpt["optim_state"])
        history.update(ckpt["history"])
        start_chunk = ckpt["chunk_idx"]
        print(f"ä» {RESUME_PATH} æ¢å¤ï¼Œå·²è®­ç»ƒåˆ° chunk {start_chunk}")

    # 4ï¸âƒ£ å¢é‡è®­ç»ƒï¼šé€ chunk è¯» CSV
    for chunk_idx, chunk in enumerate(pd.read_csv(
                                        CSV_PATH,
                                        usecols=["ex","ey","evaluation_id"],
                                        chunksize=CHUNK_SIZE,
                                        engine="python"), start=1):
        if chunk_idx <= start_chunk:
            continue

        # é‡å‘½ååˆ— & åˆå¹¶æ ‡ç­¾
        chunk = (chunk
                 .rename(columns={"ex":"x","ey":"y"})
                 .merge(diag_df, on="evaluation_id", how="inner")
                 .rename(columns={"diagnose":"label"}))
        if chunk.empty:
            print(f"Chunk {chunk_idx}ï¼šæ— æœ‰æ•ˆå¸¦æ ‡ç­¾è½¨è¿¹ï¼Œè·³è¿‡")
            continue

        print(f"\nğŸ”¹ Chunk {chunk_idx}: {len(chunk)} è¡Œ â†’ {chunk['evaluation_id'].nunique()} æ¡åºåˆ—")
        train_seqs, train_lbls_raw = build_sequences(chunk, id_col="evaluation_id")
        train_lbls = label_encoder.transform(train_lbls_raw)
        train_loader = DataLoader(TouchDataset(train_seqs, train_lbls),
                                  batch_size=BATCH_SIZE, shuffle=True)

        # â€”â€” è®­ç»ƒä¸€ä¸ª miniâ€‘epoch
        model.train()
        total_loss = 0.0
        for xb, yb in train_loader:
            xb, yb = xb.to(DEVICE), yb.to(DEVICE)
            optimizer.zero_grad()
            logits = model(xb)
            loss = criterion(logits, yb)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        avg_train_loss = total_loss / len(train_loader)

        # â€”â€” éªŒè¯
        val_loss, val_acc = evaluate(model, val_loader, criterion)
        print(f"  â†³ train_loss={avg_train_loss:.4f} | val_loss={val_loss:.4f} | val_acc={val_acc:.2f}%")

        # â€”â€” è®°å½• & checkpoint
        history["train_loss"].append(avg_train_loss)
        history["val_loss"].append(val_loss)
        history["val_acc"].append(val_acc)
        save_checkpoint(model, optimizer, chunk_idx, history)

    # 5ï¸âƒ£ å®Œæˆåç»˜å›¾
    plot_history(history, HISTORY_PLOT)
    print("è®­ç»ƒå®Œæˆã€‚æ›²çº¿å·²ä¿å­˜ï¼š", HISTORY_PLOT,
          HISTORY_PLOT.replace(".png","_acc.png"))

if __name__ == "__main__":
    main()
